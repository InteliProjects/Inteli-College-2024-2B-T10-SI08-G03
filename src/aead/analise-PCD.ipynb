{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse notebook tem o objetivo de analisar o databse \"Acompanhamento PCD\" e assim extrair insights a partir dos dados e assim contribuir para nosso projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação de bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função dessa seção do código corrigir valores nulos e inconsistências no dataset, como falta de títulos na coluna e linhas em branco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for loading and processing the Excel data, setting the correct headers\n",
    "class DataProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data_frame = None\n",
    "\n",
    "    def load_data(self):\n",
    "        temp_df = pd.read_excel(self.file_path, header=None)\n",
    "        temp_df.columns = temp_df.iloc[2]\n",
    "        self.data_frame = temp_df.drop([0, 1, 2]).reset_index(drop=True)\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data_frame\n",
    "\n",
    "\n",
    "file_path = r'C:\\Users\\Pedro Faria\\OneDrive\\Documentos\\GitHub\\2024-2B-T10-SI08-G03\\src\\dados\\Acompanhamento PCD (1).xlsx'\n",
    "data_processor = DataProcessor(file_path)\n",
    "data_processor.load_data()\n",
    "df = data_processor.get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for checking if data cleaning or preprocessing is needed\n",
    "class DataQualityChecker:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def check_null_values(self):\n",
    "        return self.data_frame.isnull().sum()\n",
    "\n",
    "    def check_duplicates(self):\n",
    "        return self.data_frame.duplicated().sum()\n",
    "\n",
    "    def check_column_types(self):\n",
    "        return self.data_frame.dtypes\n",
    "\n",
    "    def check_inconsistent_values(self):\n",
    "        inconsistent_columns = {}\n",
    "        for column in self.data_frame.columns:\n",
    "            if self.data_frame[column].dtype == 'object':\n",
    "                inconsistent_columns[column] = self.data_frame[column].value_counts()\n",
    "        return inconsistent_columns\n",
    "\n",
    "\n",
    "data_quality_checker = DataQualityChecker(df)\n",
    "\n",
    "null_values = data_quality_checker.check_null_values()\n",
    "print(\"Null values per column:\\n\", null_values)\n",
    "\n",
    "duplicates = data_quality_checker.check_duplicates()\n",
    "print(\"\\nNumber of duplicate rows:\", duplicates)\n",
    "\n",
    "column_types = data_quality_checker.check_column_types()\n",
    "print(\"\\nColumn data types:\\n\", column_types)\n",
    "\n",
    "inconsistent_values = data_quality_checker.check_inconsistent_values()\n",
    "print(\"\\nInconsistent values in categorical columns:\\n\", inconsistent_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Class for cleaning and preserving important data\n",
    "class DataCleaner:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def handle_null_values(self):\n",
    "        self.data_frame['dt_destino'] = self.data_frame['Dt Destino'].fillna('Não informado')\n",
    "        self.data_frame['dt_origem'] = self.data_frame['Dt Origem'].fillna('Não informado')\n",
    "        self.data_frame['tx_porta'] = self.data_frame['Tx Porta'].fillna('Desconhecido')\n",
    "        self.data_frame['tx_obs'] = self.data_frame['Tx Obs'].fillna('Sem Observação')\n",
    "        self.data_frame = self.data_frame.dropna(subset=['Grupos PCD'])\n",
    "        return self.data_frame\n",
    "\n",
    "    def convert_column_types(self):\n",
    "        self.data_frame['fl_alerta'] = pd.to_numeric(self.data_frame['Fl Alerta'], errors='coerce')  \n",
    "        return self.data_frame\n",
    "\n",
    "    def clean_inconsistent_values(self):\n",
    "        self.data_frame['tx_porta'] = self.data_frame['tx_porta'].replace(['1q', '01'], '1')\n",
    "        return self.data_frame\n",
    "    \n",
    "    def get_cleaned_data(self):\n",
    "        return pd.DataFrame(self.data_frame)\n",
    "    \n",
    "data_cleaner = DataCleaner(df)\n",
    "df_cleaned = data_cleaner.handle_null_values()\n",
    "df_cleaned = data_cleaner.convert_column_types()\n",
    "df_cleaned = data_cleaner.clean_inconsistent_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui iremos separar as identificaremos as colunas mais importantes e analisaremos seus valores para futuras comparações ou gereções de gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing the unique stations in the dataset\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def get_unique_stations(self):\n",
    "        unique_stations = self.data_frame['Tx Estacao Destino'].unique()\n",
    "        return unique_stations\n",
    "\n",
    "\n",
    "data_analyzer = DataAnalyzer(df)\n",
    "unique_stations = data_analyzer.get_unique_stations()\n",
    "print(\"Unique stations in the dataset:\", unique_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # This class applies basic data analysis on the loaded DataFrame\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def analyze_pcd_group(self):\n",
    "        return self.data_frame['Grupos PCD'].value_counts()\n",
    "\n",
    "    def analyze_station_dest(self):\n",
    "        return self.data_frame['Tx Estacao Destino'].value_counts()\n",
    "\n",
    "    def analyze_trains(self):\n",
    "        return self.data_frame['Tx Trem'].value_counts()\n",
    "\n",
    "    def analyze_no_alerts(self):\n",
    "        return self.data_frame[self.data_frame['Fl Alerta'] == 0].shape[0]\n",
    "\n",
    "    def analyze_dest_dates(self):\n",
    "        return self.data_frame['Dt Destino'].value_counts()\n",
    "\n",
    "\n",
    "data_analyzer = DataAnalyzer(df)\n",
    "\n",
    "pcd_group_analysis = data_analyzer.analyze_pcd_group()\n",
    "station_dest_analysis = data_analyzer.analyze_station_dest()\n",
    "train_analysis = data_analyzer.analyze_trains()\n",
    "no_alerts_count = data_analyzer.analyze_no_alerts()\n",
    "dest_dates_analysis = data_analyzer.analyze_dest_dates()\n",
    "\n",
    "print(\"Occurrences by PCD Group:\\n\", pcd_group_analysis)\n",
    "print(\"\\nOccurrences by Destination Station:\\n\", station_dest_analysis)\n",
    "print(\"\\nOccurrences by Train:\\n\", train_analysis)\n",
    "print(\"\\nCount of No-Alert Incidents:\", no_alerts_count)\n",
    "print(\"\\nDistribution by Destination Date:\\n\", dest_dates_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa conjunto de código iremos analisar as ocorrências de de PCD, assim tentando achar correlções entre as ocorrências e periodos do ano e semana, além de estações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing the PCD occurrences by month\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def analyze_pcd_by_month(self):\n",
    "        self.data_frame['dt_destino'] = pd.to_datetime(self.data_frame['Dt Destino'], errors='coerce')\n",
    "        monthly_pcd = self.data_frame.groupby(self.data_frame['dt_destino'].dt.month).size()\n",
    "        return monthly_pcd\n",
    "\n",
    "    def plot_pcd_by_month(self, monthly_pcd):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        monthly_pcd.plot(kind='bar', color='blue')\n",
    "        plt.title('PCD Occurrences by Month')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Number of PCD Occurrences')\n",
    "        plt.xticks(ticks=range(12), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "data_analyzer = DataAnalyzer(df)\n",
    "monthly_pcd_analysis = data_analyzer.analyze_pcd_by_month()\n",
    "data_analyzer.plot_pcd_by_month(monthly_pcd_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing the volume of PCD by station\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def analyze_pcd_by_station(self):\n",
    "        station_pcd = self.data_frame['Id Estacao Origem'].value_counts()\n",
    "        return station_pcd\n",
    "\n",
    "    def plot_pcd_by_station(self, station_pcd):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        station_pcd.plot(kind='bar', color='green')\n",
    "        plt.title('PCD Volume by Station')\n",
    "        plt.xlabel('Station')\n",
    "        plt.ylabel('Number of PCD Occurrences')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "data_analyzer = DataAnalyzer(df)\n",
    "station_pcd_analysis = data_analyzer.analyze_pcd_by_station()\n",
    "data_analyzer.plot_pcd_by_station(station_pcd_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing PCD occurrences by day of the week\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def analyze_pcd_by_day_of_week(self):\n",
    "        self.data_frame['dt_destino'] = pd.to_datetime(self.data_frame['Dt Destino'], errors='coerce')\n",
    "        self.data_frame['day_of_week'] = self.data_frame['dt_destino'].dt.dayofweek\n",
    "        day_of_week_pcd = self.data_frame.groupby('day_of_week').size()\n",
    "        return day_of_week_pcd\n",
    "\n",
    "    def plot_pcd_by_day_of_week(self, day_of_week_pcd):\n",
    "        day_of_week_order = ['Domingo', 'Segunda-feira', 'Terça-feira', 'Quarta-feira', 'Quinta-feira', 'Sexta-feira', 'Sábado']\n",
    "        day_of_week_pcd.index = day_of_week_order\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        day_of_week_pcd.plot(kind='bar', color='orange')\n",
    "        plt.title('Ocorrências de PCD por Dia da Semana')\n",
    "        plt.xlabel('Dia da Semana')\n",
    "        plt.ylabel('Número de Ocorrências de PCD')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "data_analyzer = DataAnalyzer(df)\n",
    "pcd_day_of_week_analysis = data_analyzer.analyze_pcd_by_day_of_week()\n",
    "data_analyzer.plot_pcd_by_day_of_week(pcd_day_of_week_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui será gerado um gráfico para descobrir os grupos PCD que mais aparecem com as ocorrências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing PCD occurrences by group\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def analyze_pcd_by_group(self):\n",
    "        group_pcd = self.data_frame['Grupos PCD'].value_counts()\n",
    "        return group_pcd\n",
    "\n",
    "    def plot_pcd_by_group(self, group_pcd):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        group_pcd.plot(kind='bar', color='purple')\n",
    "        plt.title('PCD Occurrences by Group')\n",
    "        plt.xlabel('PCD Group')\n",
    "        plt.ylabel('Number of PCD Occurrences')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "data_analyzer = DataAnalyzer(df)\n",
    "pcd_group_analysis = data_analyzer.analyze_pcd_by_group()\n",
    "data_analyzer.plot_pcd_by_group(pcd_group_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui irei começar a tentar achar relações entre os valores do alerta (1 ou 0). Concluindo que alertas que tem o valor 1 são mais graves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing PCD occurrences with and without alerts\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def analyze_pcd_with_and_without_alerts(self):\n",
    "        pcd_with_alerts = self.data_frame[self.data_frame['Fl Alerta'] == 1].shape[0]\n",
    "        pcd_without_alerts = self.data_frame[self.data_frame['Fl Alerta'] == 0].shape[0]\n",
    "        return {'With Alerts': pcd_with_alerts, 'Without Alerts': pcd_without_alerts}\n",
    "\n",
    "    def plot_pcd_with_and_without_alerts(self, alert_data):\n",
    "        labels = list(alert_data.keys())\n",
    "        values = list(alert_data.values())\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90, colors=['lightcoral', 'skyblue'])\n",
    "        plt.title('PCD Occurrences With and Without Alerts')\n",
    "        plt.show()\n",
    "\n",
    "data_analyzer = DataAnalyzer(df)\n",
    "alert_data = data_analyzer.analyze_pcd_with_and_without_alerts()\n",
    "data_analyzer.plot_pcd_with_and_without_alerts(alert_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyzes the 'fl_alerta' column and related data\n",
    "class AlertAnalyzer:\n",
    "    \"\"\"Analyzes the 'fl_alerta' column and related data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "        self.data_frame.columns = self.data_frame.columns.str.strip()\n",
    "\n",
    "    def get_alert_counts(self):\n",
    "        return self.data_frame['fl_alerta'].value_counts()\n",
    "\n",
    "    def get_alert_percentages(self):\n",
    "        return self.data_frame['fl_alerta'].value_counts(normalize=True) * 100\n",
    "\n",
    "    def filter_alerts(self, alert_value=1):\n",
    "        return self.data_frame[self.data_frame['fl_alerta'] == alert_value]\n",
    "\n",
    "    def analyze_pcd_groups(self, alert_df):\n",
    "        return alert_df['Grupos PCD'].value_counts()\n",
    "\n",
    "    def analyze_origin_stations(self, alert_df):\n",
    "        return alert_df['Id Estacao Origem'].value_counts()\n",
    "\n",
    "    def analyze_alerts_by_hour(self, alert_df):\n",
    "        alert_df['Dt Origem'] = pd.to_datetime(alert_df['dt_origem'], errors='coerce')\n",
    "        alert_df['hour_origin'] = alert_df['Dt Origem'].dt.hour\n",
    "        return alert_df['hour_origin'].value_counts().sort_index()\n",
    "\n",
    "    def plot_alert_distribution(self):\n",
    "        sns.countplot(x='fl_alerta', data=self.data_frame)\n",
    "        plt.title('Distribution of fl_alerta values')\n",
    "        plt.xlabel('fl_alerta')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_pcd_groups(self, alert_df):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.countplot(y='Grupos PCD', data=alert_df, order=alert_df['Grupos PCD'].value_counts().index)\n",
    "        plt.title('PCD Groups with fl_alerta == 1')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Grupos PCD')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_alerts_by_hour(self, alert_counts_by_hour):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        alert_counts_by_hour.plot(kind='line', marker='o')\n",
    "        plt.title('Alerts with fl_alerta == 1 by Hour')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.ylabel('Number of Alerts')\n",
    "        plt.xticks(range(0,24))\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def get_alert_observations(self, alert_df):\n",
    "        return alert_df['tx_obs'].unique()\n",
    "\n",
    "    def get_numeric_correlations(self):\n",
    "        numeric_cols = self.data_frame.select_dtypes(include=['float64', 'int64']).columns\n",
    "        return self.data_frame[numeric_cols].corr()['fl_alerta']\n",
    "\n",
    "alert_analyzer = AlertAnalyzer(df_cleaned)\n",
    "\n",
    "alert_counts = alert_analyzer.get_alert_counts()\n",
    "alert_percentages = alert_analyzer.get_alert_percentages()\n",
    "df_alerts_1 = alert_analyzer.filter_alerts(alert_value=1)\n",
    "pcd_group_counts = alert_analyzer.analyze_pcd_groups(df_alerts_1)\n",
    "origin_station_counts = alert_analyzer.analyze_origin_stations(df_alerts_1)\n",
    "alert_counts_by_hour = alert_analyzer.analyze_alerts_by_hour(df_alerts_1)\n",
    "alert_analyzer.plot_alert_distribution()\n",
    "alert_analyzer.plot_pcd_groups(df_alerts_1)\n",
    "alert_analyzer.plot_alerts_by_hour(alert_counts_by_hour)\n",
    "alert_observations = alert_analyzer.get_alert_observations(df_alerts_1)\n",
    "alert_correlations = alert_analyzer.get_numeric_correlations()\n",
    "\n",
    "print(\"Alert counts:\\n\", alert_counts)\n",
    "print(\"\\nAlert percentages:\\n\", alert_percentages)\n",
    "print(\"\\nPCD group counts in alerts:\\n\", pcd_group_counts)\n",
    "print(\"\\nOrigin station counts in alerts:\\n\", origin_station_counts)\n",
    "print(\"\\nAlert counts by hour:\\n\", alert_counts_by_hour)\n",
    "print(\"\\nUnique observations in alerts:\\n\", alert_observations)\n",
    "print(\"\\nCorrelations with 'fl_alerta':\\n\", alert_correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing alerts in the dataset\n",
    "class AlertAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "        self.data_frame.columns = self.data_frame.columns.str.strip()\n",
    "\n",
    "    def filter_alerts(self, alert_value=1):\n",
    "        return self.data_frame[self.data_frame['fl_alerta'] == alert_value]\n",
    "\n",
    "    def get_stations_with_most_alerts(self, alert_df):\n",
    "        station_alert_counts = alert_df['Id Estacao Origem'].value_counts()\n",
    "        return station_alert_counts\n",
    "\n",
    "    def plot_stations_with_most_alerts(self, station_alert_counts, top_n=10):\n",
    "        top_stations = station_alert_counts.head(top_n)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=top_stations.values, y=top_stations.index, orient='h')\n",
    "        plt.title(f'Top {top_n} Stations with Most Alerts (fl_alerta == 1)')\n",
    "        plt.xlabel('Number of Alerts')\n",
    "        plt.ylabel('Station ID')\n",
    "        plt.show()\n",
    "\n",
    "alert_analyzer = AlertAnalyzer(df_cleaned)\n",
    "df_alerts_1 = alert_analyzer.filter_alerts(alert_value=1)\n",
    "station_alert_counts = alert_analyzer.get_stations_with_most_alerts(df_alerts_1)\n",
    "print(\"Stations with most alerts (fl_alerta == 1):\\n\", station_alert_counts)\n",
    "alert_analyzer.plot_stations_with_most_alerts(station_alert_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for analyzing alerts by destination stations using 'Tx Estacao Destino'\n",
    "class DestinationAlertAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "        self.data_frame.columns = self.data_frame.columns.str.strip()\n",
    "\n",
    "    def filter_alerts(self, alert_value=1):\n",
    "        return self.data_frame[self.data_frame['fl_alerta'] == alert_value]\n",
    "\n",
    "    def get_destination_stations_with_most_alerts(self, alert_df):\n",
    "        station_alert_counts = alert_df['Tx Estacao Destino'].value_counts()\n",
    "        return station_alert_counts\n",
    "\n",
    "    def plot_destination_stations_with_most_alerts(self, station_alert_counts, top_n=10):\n",
    "        top_stations = station_alert_counts.head(top_n)\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(x=top_stations.values, y=top_stations.index, orient='h')\n",
    "        plt.title(f'Top {top_n} Destination Stations with Most Alerts (fl_alerta == 1)')\n",
    "        plt.xlabel('Number of Alerts')\n",
    "        plt.ylabel('Destination Station')\n",
    "        plt.show()\n",
    "\n",
    "alert_analyzer = DestinationAlertAnalyzer(df_cleaned)\n",
    "df_alerts_1 = alert_analyzer.filter_alerts(alert_value=1)\n",
    "station_alert_counts = alert_analyzer.get_destination_stations_with_most_alerts(df_alerts_1)\n",
    "print(\"Destination stations with most alerts (fl_alerta == 1):\\n\", station_alert_counts)\n",
    "alert_analyzer.plot_destination_stations_with_most_alerts(station_alert_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa parte iremos avaliar o volume (agora de forma visual) para entender melhor o volume de ocorrências de cada grupo nas estações. Esta sendo considerado separadamente as estações de origem e destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes the volume of each PCD type per origin station\n",
    "class PCDVolumeAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def compute_pcd_volume_per_station(self):\n",
    "        pcd_station_counts = self.data_frame.groupby(['Grupos PCD', 'Id Estacao Origem']).size().reset_index(name='count')\n",
    "        return pcd_station_counts\n",
    "\n",
    "    def plot_pcd_volume_per_station(self, pcd_station_counts, pcd_type):\n",
    "        data = pcd_station_counts[pcd_station_counts['Grupos PCD'] == pcd_type]\n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.barplot(x='Id Estacao Origem', y='count', data=data)\n",
    "        plt.title(f'Volume of {pcd_type} per Station')\n",
    "        plt.xlabel('Station')\n",
    "        plt.ylabel('Volume')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "pcd_analyzer = PCDVolumeAnalyzer(df_cleaned)\n",
    "pcd_station_counts = pcd_analyzer.compute_pcd_volume_per_station()\n",
    "pcd_types = ['Visual', 'Cadeira Rodas', 'Mob Reduzida', '60+', 'Autista', 'Gestante', 'Oncológico', 'Obeso']\n",
    "\n",
    "for pcd_type in pcd_types:\n",
    "    pcd_analyzer.plot_pcd_volume_per_station(pcd_station_counts, pcd_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes the volume of each PCD type per destination station\n",
    "class PCDVolumeAnalyzer:\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "\n",
    "    def compute_pcd_volume_per_station(self):\n",
    "        pcd_station_counts = self.data_frame.groupby(['Grupos PCD', 'Id Estacao Origem']).size().reset_index(name='count')\n",
    "        return pcd_station_counts\n",
    "\n",
    "    def plot_pcd_volume_per_station(self, pcd_station_counts, pcd_type):\n",
    "        data = pcd_station_counts[pcd_station_counts['Grupos PCD'] == pcd_type]\n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.barplot(x='Id Estacao Origem', y='count', data=data)\n",
    "        plt.title(f'Volume of {pcd_type} per Station')\n",
    "        plt.xlabel('Station')\n",
    "        plt.ylabel('Volume')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "pcd_analyzer = PCDVolumeAnalyzer(df_cleaned)\n",
    "pcd_station_counts = pcd_analyzer.compute_pcd_volume_per_station()\n",
    "pcd_types = ['Visual', 'Cadeira Rodas', 'Mob Reduzida', '60+', 'Autista', 'Gestante', 'Oncológico', 'Obeso']\n",
    "\n",
    "for pcd_type in pcd_types:\n",
    "    pcd_analyzer.plot_pcd_volume_per_station(pcd_station_counts, pcd_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base na análise realizada, os seguintes insights foram identificados:\n",
    "\n",
    "1. **Estações com Maior Volume de Ocorrências**: As estações com os IDs 64, 9, 13, 94, 16 e 41 apresentam o maior número de ocorrências envolvendo PCD. Isso indica a necessidade de atenção especial nessas localidades para melhorar a acessibilidade e o suporte oferecido.\n",
    "\n",
    "2. **Meses com Mais Ocorrências**: Os meses de **junho**, **julho** e **agosto** registram um aumento significativo no número de ocorrências.\n",
    "\n",
    "3. **Horário de Pico das Ocorrências**: O maior volume de ocorrências ocorre entre as **16h e 19h**, correspondendo ao horário de pico. Isso destaca a importância de reforçar o suporte e a assistência às PCD durante esse intervalo de tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão para parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.parquet import Conversor\n",
    "\n",
    "conversor = Conversor()\n",
    "\n",
    "conversor.df_to_parquet(data_cleaner.get_cleaned_data(), \"./Acompanhamento_PCD.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload para o S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.aws_conn import AwsConn\n",
    "\n",
    "aws_conn = AwsConn()\n",
    "\n",
    "aws_conn.send_to_s3(\"acompanhamento_pcd.parquet\", \"big-data-PCD\", \"./acompanhamento_pcd.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-FFmuh1Kz-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
